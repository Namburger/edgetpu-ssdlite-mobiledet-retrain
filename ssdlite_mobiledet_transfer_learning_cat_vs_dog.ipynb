{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssdlite_mobiledet_transfer_learning_cat_vs_dog",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namburger/edgetpu-ssdlite-mobiledet-retrain/blob/master/ssdlite_mobiledet_transfer_learning_cat_vs_dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvEMJSafnyEC",
        "colab_type": "text"
      },
      "source": [
        "**Retrain SSDLite Mobiledet for Coral's EdgeTpu**\n",
        "\n",
        "* The paper published on MobileDet+SSDLite was submitted in April 2020: https://arxiv.org/pdf/2004.14525.pdf\n",
        "* It out performs MobileNetV3+SSDLite by 1.7 mAP and MobilenetV2+SSDLite by 1.9 mAP on CPU and 2.7 mAP at comparable latency on CPU and up to 2x speedup on the EdgeTPU\n",
        "* This colab shows a step by step guide to retrain the model for the Coral's EdgeTpu using google's GPU for free\n",
        "* The data set we will be using is the Oxford-IIIT Pet dataset, total of 36 different classes of variaous dog and cat breeds. However, for this tutorial, only 2 classes will be used for training. The produced model should easily distinguised between a cat can a dog."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YetL1g47nyec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import tensorflow 1.x and install tf_slim.\n",
        "%tensorflow_version 1.x\n",
        "!pip install tf_slim\n",
        "!pip show tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnSBXH75XMcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install protobuf-compiler and the tensorflow's object detection API.\n",
        "!apt-get install protobuf-compiler\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/slim/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/object_detection/utils/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/object_detection'\n",
        "\n",
        "%cd models/research\n",
        "# Compile all the protobuf dependencies.\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "# Set up and install the object detection API.\n",
        "!cp object_detection/packages/tf1/setup.py .\n",
        "!python -m pip install .\n",
        "# Run a test to make sure setup is correct.\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-7dJozsupa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's download our training dataset.\n",
        "%mkdir /content/dataset\n",
        "%cd /content/dataset\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar zxf images.tar.gz\n",
        "!tar zxf annotations.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSpk4nRTHAIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only picking Abyssinian Cat and the American Bulldog\n",
        "# If you wish to train the model on all classes, simply skip this entire cell.\n",
        "!cp /content/dataset/annotations/list.txt /content/dataset/annotations/list_petsdataset.txt\n",
        "!cp /content/dataset/annotations/trainval.txt /content/dataset/annotations/trainval_petsdataset.txt\n",
        "!cp /content/dataset/annotations/test.txt /content/dataset/annotations/test_petsdataset.txt\n",
        "!grep \"Abyssinian\" /content/dataset/annotations/list_petsdataset.txt >  /content/dataset/annotations/list.txt\n",
        "!grep \"american_bulldog\" /content/dataset/annotations/list_petsdataset.txt >> /content/dataset/annotations/list.txt\n",
        "!grep \"Abyssinian\" /content/dataset/annotations/trainval_petsdataset.txt > /content/dataset/annotations/trainval.txt\n",
        "!grep \"american_bulldog\" /content/dataset/annotations/trainval_petsdataset.txt >> /content/dataset/annotations/trainval.txt\n",
        "!grep \"Abyssinian\" /content/dataset/annotations/test_petsdataset.txt > /content/dataset/annotations/test.txt\n",
        "!grep \"american_bulldog\" /content/dataset/annotations/test_petsdataset.txt >> /content/dataset/annotations/test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdF1oqYWuL4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can create the tfrecord files.\n",
        "%cd /content/models/research\n",
        "!cp object_detection/data/pet_label_map.pbtxt /content/dataset\n",
        "!python3 object_detection/dataset_tools/create_pet_tf_record.py \\\n",
        "    --label_map_path=\"/content/dataset/pet_label_map.pbtxt\" \\\n",
        "    --data_dir=\"/content/dataset\" \\\n",
        "    --output_dir=\"/content/dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5v347uRtKzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's download our ssdlite mobiledet pretrained model from tensorflow's model zoo.\n",
        "!mkdir /content/pretrained_model\n",
        "%cd /content/pretrained_model\n",
        "!wget http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19.tar.gz\n",
        "!tar xvf ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg1C8UwStK7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Edit Pipeline config to load in our new tfrecord that we just created and add quantization aware training.\n",
        "import tensorflow as tf\n",
        "from google.protobuf import text_format\n",
        "from object_detection.protos import pipeline_pb2\n",
        "\n",
        "# Hack to find out if you have colab pro or not :)\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "print(gpu_info)\n",
        "gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "# You get Tesla T4 with free colab and Tesla P100-PCIe with colab pro.\n",
        "colab_pro = False if 'T4' in gpu_name else True\n",
        "\n",
        "pipeline = pipeline_pb2.TrainEvalPipelineConfig()                                                                                                                                                                                                          \n",
        "config_path = '/content/models/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config'\n",
        "with tf.gfile.GFile(config_path, \"r\") as f:                                                                                                                                                                                                                     \n",
        "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
        "    text_format.Merge(proto_str, pipeline)\n",
        "\n",
        "pipeline.train_input_reader.tf_record_input_reader.input_path[:] = ['/content/dataset/pet_faces_train.record-?????-of-00010']\n",
        "pipeline.train_input_reader.label_map_path = '/content/dataset/pet_label_map.pbtxt'\n",
        "pipeline.eval_input_reader[0].tf_record_input_reader.input_path[:] = ['/content/dataset/pet_faces_val.record-?????-of-00010']\n",
        "pipeline.eval_input_reader[0].label_map_path = '/content/dataset/pet_label_map.pbtxt'\n",
        "pipeline.train_config.fine_tune_checkpoint = '/content/pretrained_model/ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19/fp32/model.ckpt'\n",
        "pipeline.train_config.batch_size = 64 if colab_pro else 32 # Smaller batch size on free gpu to avoid OOM Killer\n",
        "pipeline.train_config.num_steps = 25000 if colab_pro else 10000 # Less steps with free gpu but 10k should be good enough\n",
        "pipeline.model.ssd.num_classes = 2\n",
        "# Enable ssdlite, this should already be enabled in the config we downloaded, but this is just to make sure.\n",
        "pipeline.model.ssd.box_predictor.convolutional_box_predictor.kernel_size = 3\n",
        "pipeline.model.ssd.box_predictor.convolutional_box_predictor.use_depthwise = True\n",
        "pipeline.model.ssd.feature_extractor.use_depthwise = True\n",
        "# Quantization Aware Training\n",
        "pipeline.graph_rewriter.quantization.delay = 0\n",
        "pipeline.graph_rewriter.quantization.weight_bits = 8\n",
        "pipeline.graph_rewriter.quantization.activation_bits = 8\n",
        "\n",
        "config_text = text_format.MessageToString(pipeline)                                                                                                                                                                                                        \n",
        "with tf.gfile.Open(config_path, \"wb\") as f:                                                                                                                                                                                                                       \n",
        "    f.write(config_text)\n",
        "\n",
        "# This is out config after modifying.\n",
        "!cat /content/models/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whIM6gZz2DEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Before we start training, let's start tensorboard so we can track the progress.\n",
        "# More info on tensorflow can be found here: https://www.tensorflow.org/tutorials\n",
        "%cd /content\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvJ9X0qBfSIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starts tensorboard, so we can monitor the training process.\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format('/content/train')\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "print('Click on link below to track progress:')\n",
        "import time\n",
        "time.sleep(1)\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idw8OgZQtLKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's begin training, expects to take a few hours, time for a good stretch :)\n",
        "%cd /content/models/research/\n",
        "!python3 object_detection/model_main.py \\\n",
        "    --logtostderr=true \\\n",
        "    --model_dir=/content/train \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KicA9DZZmnKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make inference graph.\n",
        "!python3 /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config \\\n",
        "    --output_directory=/content/inference_graph \\\n",
        "    --trained_checkpoint_prefix=/content/train/model.ckpt-25000 # Make sure to change this checkpoint to the corresponding num step you set from above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRRXF0TrMqHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's download some test data from flickr.\n",
        "!mkdir /content/test\n",
        "!cd /content/test\n",
        "!wget https://live.staticflickr.com/7921/46683787864_86c9501c24_c_d.jpg -O /content/test/image1.jpg\n",
        "!wget https://live.staticflickr.com/4/8451898_8bedb2ae53_c_d.jpg -O /content/test/image2.jpg\n",
        "!wget https://live.staticflickr.com/2654/3997966238_f454845087_c_d.jpg -O /content/test/image3.jpg\n",
        "!wget https://live.staticflickr.com/2818/34032378096_5309537c9f_c_d.jpg -O /content/test/image4.jpg\n",
        "!wget https://live.staticflickr.com/8682/28214087384_4c7711584d_c_d.jpg -O /content/test/image5.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhg-2OA-m-9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do a Quick Evaluation on the inference graph model.\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "%matplotlib inline\n",
        "\n",
        "# Initialize tf.Graph()\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile('/content/inference_graph/frozen_inference_graph.pb', 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "# Loads labels\n",
        "label_map = label_map_util.load_labelmap('/content/dataset/pet_label_map.pbtxt')\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=2, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Run Inference and populates results in a dict.\n",
        "def run_inference(graph, image):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = [output.name for op in ops for output in op.outputs]\n",
        "      tensor_dict = {}\n",
        "      tensor_keys = ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes']\n",
        "      for key in tensor_keys:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
        "      \n",
        "      # Actual inference.\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "      output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "  return output_dict\n",
        "\n",
        "test_image_path = [os.path.join('/content/test', 'image{}.jpg'.format(i)) for i in range(1, 6)]\n",
        "for image_path in test_image_path:\n",
        "  print('Evaluating:', image_path)\n",
        "  image = Image.open(image_path)\n",
        "  img_width, img_height = image.size\n",
        "  image_np = np.array(image.getdata()).reshape((img_height, img_width, 3)).astype(np.uint8)\n",
        "  # Run inference.\n",
        "  output_dict = run_inference(detection_graph, image_np)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sHP33iZBlil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we export this model to tflite_graph format.\n",
        "%cd /content/models/research\n",
        "!mkdir /content/output_model\n",
        "!python3 object_detection/export_tflite_ssd_graph.py \\\n",
        "  --pipeline_config_path=/content/models/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config \\\n",
        "  --trained_checkpoint_prefix=/content/train/model.ckpt-25000 \\\n",
        "  --output_directory=/content/output_model \\\n",
        "  --add_postprocessing_op=true\n",
        "# Make sure to change the model-ckpt-# to match the checkpoint number you used."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NYcwbhdEmBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we can convert this custom trained model to a CPU tflite model\n",
        "!tflite_convert \\\n",
        "  --output_file=\"/content/output_model/ssdlite_mobiledet_dog_vs_cat.tflite\" \\\n",
        "  --graph_def_file=\"/content/output_model/tflite_graph.pb\" \\\n",
        "  --inference_type=QUANTIZED_UINT8 \\\n",
        "  --input_arrays=\"normalized_input_image_tensor\" \\\n",
        "  --output_arrays=\"TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\" \\\n",
        "  --mean_values=128 \\\n",
        "  --std_dev_values=128 \\\n",
        "  --input_shapes=1,320,320,3 \\\n",
        "  --allow_custom_ops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isTD5FWZ0K6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install tflite_runtime package to evaluate the model.\n",
        "!pip3 install https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A-IYRpm-Ate",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we do evaluation on the tflite model.\n",
        "import os\n",
        "import numpy as np\n",
        "from tflite_runtime.interpreter import Interpreter\n",
        "from tflite_runtime.interpreter import load_delegate\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "%matplotlib inline\n",
        "\n",
        "# Creates tflite interpreter\n",
        "interpreter = Interpreter('/content/output_model/ssdlite_mobiledet_dog_vs_cat.tflite')\n",
        "# This exact code can be used to run inference on the edgetpu by simply creating \n",
        "# the instantialize the interpreter with libedgetpu delegates:\n",
        "# interpreter = Interpreter(args.model, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\n",
        "interpreter.allocate_tensors()\n",
        "interpreter.invoke() # warmup\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "width = input_details[0]['shape'][2]\n",
        "height = input_details[0]['shape'][1]\n",
        "\n",
        "def run_inference(interpreter, image):\n",
        "  interpreter.set_tensor(input_details[0]['index'], image)\n",
        "  interpreter.invoke()\n",
        "  boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "  classes = interpreter.get_tensor(output_details[1]['index'])[0]\n",
        "  scores = interpreter.get_tensor(output_details[2]['index'])[0]\n",
        "  # num_detections = interpreter.get_tensor(output_details[3]['index'])[0]\n",
        "  return boxes, classes, scores\n",
        "\n",
        "test_image_paths = [os.path.join('/content/test', 'image{}.jpg'.format(i)) for i in range(1, 6)]\n",
        "for image_path in test_image_paths:\n",
        "  print('Evaluating:', image_path)\n",
        "  image = Image.open(image_path)\n",
        "  image_width, image_height = image.size\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  resized_image = image.resize((width, height))\n",
        "  np_image = np.asarray(resized_image)\n",
        "  input_tensor = np.expand_dims(np_image, axis=0)\n",
        "  # Run inference\n",
        "  boxes, classes, scores = run_inference(interpreter, input_tensor)\n",
        "  # Draw results on image\n",
        "  colors = {0:(128, 255, 102), 1:(102, 255, 255)}\n",
        "  labels = {0:'abyssian cat', 1:'american bulldog'}\n",
        "  for i in range(len(boxes)):\n",
        "    if scores[i] > .7:\n",
        "      ymin = int(max(1, (boxes[i][0] * image_height)))\n",
        "      xmin = int(max(1, (boxes[i][1] * image_width)))\n",
        "      ymax = int(min(image_height, (boxes[i][2] * image_height)))\n",
        "      xmax = int(min(image_width, (boxes[i][3] * image_width)))\n",
        "      draw.rectangle((xmin, ymin, xmax, ymax), width=7, outline=colors[int(classes[i])])\n",
        "      draw.rectangle((xmin, ymin, xmax, ymin-10), fill=colors[int(classes[i])])\n",
        "      text = labels[int(classes[i])] + ' ' + str(scores[i]*100) + '%'\n",
        "      draw.text((xmin+2, ymin-10), text, fill=(0,0,0), width=2)\n",
        "  display(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUy2l-pJFUfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the edgetpu compiler.\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install edgetpu-compiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW45-68aFbNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile our model and make a tarball of the finished trained model.\n",
        "%cd /content/output_model\n",
        "!edgetpu_compiler -s ssdlite_mobiledet_custom.tflite\n",
        "%cd /content/\n",
        "# Copy the checkpoints, inference graph, pipeline config, and the tflite models.\n",
        "!cp -r /content/train/model.ckpt-50000* /content/output_model\n",
        "!cp -r /content/inference_graph/* /content/output_model\n",
        "!tar cvf ssdlite_mobiledet_dog_vs_cat.tar.gz output_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b13V2T5wfG5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download model and you're done!\n",
        "from google.colab import files\n",
        "files.download('/content/ssdlite_mobiledet_dog_vs_cat.tar.gz')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
